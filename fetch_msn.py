#!/usr/bin/env python3
import requests
import json
import os
import unicodedata
from datetime import datetime
from bs4 import BeautifulSoup

# Lista √∂ver orter
cities = ["eskilstuna", "stockholm", "g√∂teborg", "lomma", "malm√∂", "ume√•"]

OUTPUT_FOLDER = "data"
os.makedirs(OUTPUT_FOLDER, exist_ok=True)

def slugify(city):
    """Ta bort diakritiska tecken √•√§√∂ ‚Üí aao osv, f√∂r engelsk fallback."""
    nfkd = unicodedata.normalize("NFKD", city)
    return "".join(c for c in nfkd if not unicodedata.combining(c)).lower()

def find_swedish_url(en_html):
    """Hitta <link rel="alternate" hreflang="sv-se"> i engelsk sida."""
    soup = BeautifulSoup(en_html, "html.parser")
    tag = soup.find("link", {"rel": "alternate", "hreflang": "sv-se"})
    return tag["href"] if tag and tag.has_attr("href") else None

def fetch_msn_html(city):
    """H√§mta HTML fr√•n svenska MSN om m√∂jligt, annars engelsk sida."""
    slug = slugify(city)
    en_url = f"https://www.msn.com/en-us/weather/{slug}"
    print(f"üå¶Ô∏è H√§mtar engelska MSN f√∂r {city}: {en_url}")
    r = requests.get(en_url)
    if r.status_code != 200:
        print(f"‚ö†Ô∏è Engelska sidan gav {r.status_code}, hoppar √∂ver")
        return None

    sv_url = find_swedish_url(r.text)
    if sv_url:
        print(f"üå¶Ô∏è Byter till svenska MSN-URL: {sv_url}")
        r2 = requests.get(sv_url)
        if r2.status_code == 200:
            return r2.text
        print(f"‚ö†Ô∏è Svenska sidan gav {r2.status_code}, anv√§nder engelska")
    return r.text

def parse_and_save(city, html):
    soup = BeautifulSoup(html, "html.parser")
    # Loopar alla script[type=application/json] och s√∂ker efter "forecasts"
    scripts = soup.find_all("script", {"type": "application/json"})
    js = None
    for tag in scripts:
        txt = tag.string or ""
        if '"forecasts"' in txt:
            try:
                js = json.loads(txt)
                break
            except json.JSONDecodeError:
                continue

    if not js:
        print(f"‚ö†Ô∏è Hittade ingen forecasts‚ÄêJSON f√∂r {city}, skippar.")
        return

    # Extrahera tim‚Äêlista
    hourly = (
        js.get("props", {})
          .get("pageProps", {})
          .get("forecasts", {})
          .get("hourly", None)
        or js.get("forecasts", {}).get("hourly", [])
    )
    if not isinstance(hourly, list):
        print(f"‚ö†Ô∏è Ingen hourly‚Äêlista f√∂r {city}, skippar.")
        return

    out = []
    for h in hourly:
        iso = h.get("dateTime")
        if not iso:
            continue
        dt = datetime.fromisoformat(iso)
        out.append({
            "time": dt.strftime("%Y-%m-%d %H:%M:%S"),
            "temp": h.get("temperature"),
            "desc": h.get("iconPhrase", "").lower()
        })

    path = os.path.join(OUTPUT_FOLDER, f"msn_{city}.json")
    with open(path, "w", encoding="utf-8") as f:
        json.dump(out, f, ensure_ascii=False, indent=2)
    print(f"‚úÖ Sparat: {path} ({len(out)} datapunkter)")

if __name__ == "__main__":
    for city in cities:
        html = fetch_msn_html(city)
        if html:
            parse_and_save(city, html)
